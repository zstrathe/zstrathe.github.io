{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataProject-PySpark_RDD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cuSuJUkQ3DG"
      },
      "source": [
        "**Notes on running this notebook:**\n",
        "- Due to the total size of the datasets (~6 GB total), this notebook should be run on a machine with high-memory.\n",
        "- I successfully ran this notebook (with very limited-testing due to cost) on a AWS EMR Spark cluster, loading the data from a S3 bucket.\n",
        "- However, most of my testing with this notebook was done with a single 16-core, 64 GB memory Google Cloud VM.\n",
        "  - This lacks the speed of running a cluster, but is cheaper, and therefore a great method for testing a PySpark implementation on a budget.\n",
        "  - In the case of running on a single machine/VM, it is important to setup the PySpark configuration to utilize all available computing cores and memory. I have done this below, using SparkConf() when initilizing the SparkContext.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkcqKfMSuzRj"
      },
      "source": [
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MotE4aaKmDRP"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import time \n",
        "import csv\n",
        "from os import path, environ\n",
        "import cv2\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning) # ignore FutureWarning from PySpark\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.mllib.feature import Normalizer, ChiSqSelector\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.tree import RandomForest, DecisionTree, GradientBoostedTrees\n",
        "from pyspark.mllib.classification import NaiveBayes, SVMWithSGD, LogisticRegressionWithLBFGS\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "''' \n",
        "Since I am running this on a single VM with 16 CPU cores and 64 GB of memory, \n",
        "the next lines update the Spark configuration to use all available resources.\n",
        "Though I did also test with an AWS EMR cluster where this configuration step wasn't necessary. \n",
        "'''\n",
        "conf = SparkConf().setAppName(\"App\")\n",
        "conf = (conf.setMaster('local[*]')\n",
        "        .set('spark.cores.max', '24')\n",
        "        .set('spark.yarn.am.cores', '24')\n",
        "        .set('spark.yarn.am.memory', '64G')\n",
        "        .set('spark.executor.memory', '64G')\n",
        "        .set('spark.driver.memory', '64G')\n",
        "        .set('spark.driver.maxResultSize', '64G'))\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE7Qy3Ja2YC3"
      },
      "source": [
        "#################\n",
        "##  Load Data  ##\n",
        "#################\n",
        "\n",
        "''' \n",
        "Loads files as Pyspark RDDs\n",
        "'''\n",
        "\n",
        "fileDir = '../../content/'\n",
        "x_train_file = path.join(fileDir, 'X_train_sat4.csv')\n",
        "y_train_file = path.join(fileDir, 'y_train_sat4.csv')\n",
        "x_test_file = path.join(fileDir, 'X_test_sat4.csv')\n",
        "y_test_file = path.join(fileDir, 'y_test_sat4.csv')\n",
        "\n",
        "x_train_orig = sqlContext.read.options(header=False, inferSchema=True).csv(x_train_file).rdd\n",
        "y_train_orig = sqlContext.read.options(header=False, inferSchema=True).csv(y_train_file).rdd\n",
        "x_test_orig = sqlContext.read.options(header=False, inferSchema=True).csv(x_test_file).rdd\n",
        "y_test_orig =sqlContext.read.options(header=False, inferSchema=True).csv(y_test_file).rdd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLHqJNaNmHYQ"
      },
      "source": [
        "############################\n",
        "##  Data Pre-Processing   ## \n",
        "##  Function Definitions  ##\n",
        "############################\n",
        "'''\n",
        "  Function to take the image labels (in RDD format) and return a single int for category\n",
        "  Labels in csv files are lists of 4 binaries with\n",
        "  ['1','0','0','0'] = 'Barren Land' ==> category 1\n",
        "  ['0','1','0','0'] = 'Trees ==> category 2\n",
        "  ['0','0','1','0'] = 'Grassland' ==> category 3\n",
        "  ['0','0','0','1'] = 'Other' ==> category 4\n",
        "'''\n",
        "def convert_label(imageLabel):\n",
        "  if imageLabel[0] == 1: \n",
        "    return 0.0\n",
        "  if imageLabel[1] == 1:\n",
        "    return 1.0\n",
        "  if imageLabel[2] == 1:\n",
        "    return 2.0\n",
        "  if imageLabel[3] == 1:\n",
        "    return 3.0\n",
        "\n",
        "def join_rdds_lp(rdd_labels, rdd_data):\n",
        "  '''\n",
        "  Input: RDD of labels, RDD of data\n",
        "  Output: RDD of LabeledPoints\n",
        "  '''\n",
        "  rdd_labels = rdd_labels.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
        "  rdd_data = rdd_data.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
        "\n",
        "  # Join labels and data into a single LabeledPoint RDD that can be passed to a Spark MLlib (RDD-based) classification algorithm\n",
        "  combined = rdd_labels.join(rdd_data).sortBy(lambda x: x[0]).map(lambda x: x[1]) # get only the label and data tuple after joining / get rid of the index, after sorting by index\n",
        "  combined = combined.map(lambda x: LabeledPoint(x[0], Vectors.dense(x[1]))) # map to LabeledPoint format for MLlib RDD-based algs\n",
        "  return combined\n",
        "\n",
        "def pixels_transform(imageDataRow, rgb_mean=False, add_infra=False, remove_infra=False, infra_only=False):\n",
        "  row_output = []\n",
        "  img_reshaped = np.reshape(imageDataRow,(784,4))\n",
        "\n",
        "  if rgb_mean:\n",
        "    for pixel_group in img_reshaped:\n",
        "      tmp_sum = 0\n",
        "      for num in range(3): # only iterate through the 3 RGB values / ignore the 4th value (infrared)\n",
        "        tmp_sum += pixel_group[num]\n",
        "      row_output.append(float(tmp_sum/3))\n",
        "      if add_infra: \n",
        "        row_output.append(float(pixel_group[-1]))\n",
        "    return tuple(row_output)\n",
        "\n",
        "  if infra_only:\n",
        "    for pixel_group in img_reshaped:\n",
        "      row_output.append(float(pixel_group[-1]))\n",
        "    return tuple(row_output)\n",
        "\n",
        "  if remove_infra:\n",
        "    for pixel_group in img_reshaped:\n",
        "      for num in range(3):\n",
        "        row_output.append(pixel_group[num])\n",
        "    return tuple(row_output)\n",
        "    \n",
        "  pass  \n",
        "\n",
        "def cv2_transforms(img, sobel_edge=False, sobel_plus_all=False, hu_moments=False, hu_moments_plus_all=False, histogram_greyscale=False, histogram_greyscale_plus_all=False):\n",
        "  '''\n",
        "  Input: a row from a RDD (in a map transformation)\n",
        "  Output: a list (back to an RDD in a map transformation)\n",
        "  '''\n",
        "  img_rgb = pixels_transform(img, remove_infra=True)\n",
        "  img_rgb = np.reshape(img_rgb,(28,28,3))\n",
        "  img_rgb = np.array(img_rgb, 'uint8')\n",
        "  # Convert to graycsale\n",
        "  img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
        "\t\n",
        "  if sobel_edge:\n",
        "    # Sobel Edge Detection\n",
        "    sobelxy = cv2.Sobel(src=img_gray, dst=cv2.CV_64F, ddepth=-1, dx=1, dy=1, ksize=1)\n",
        "    sobelxy = sobelxy.flatten()\n",
        "    return sobelxy.tolist()\n",
        "\n",
        "  if sobel_plus_all:\n",
        "    sobelxy = cv2.Sobel(src=img_gray, dst=cv2.CV_64F, ddepth=-1, dx=1, dy=1, ksize=1)\n",
        "    sobelxy = sobelxy.flatten()\n",
        "    img = np.reshape(img, (784,4)).tolist()\n",
        "    for i in range(784):\n",
        "      img[i].append(sobelxy[i])\n",
        "    img = np.array(img).flatten()\n",
        "    return img.tolist()\n",
        "\n",
        "  if hu_moments:\n",
        "    features = cv2.HuMoments(cv2.moments(img_gray)).flatten()\n",
        "    return features.tolist()\n",
        "\n",
        "  if hu_moments_plus_all:\n",
        "    features = cv2.HuMoments(cv2.moments(img_gray)).flatten()\n",
        "    img = np.reshape(img, (784*4,)).tolist()\n",
        "    [img.append(i) for i in features]\n",
        "    img = np.array(img).flatten().tolist()\n",
        "    return img\n",
        "\n",
        "  if histogram_greyscale:\n",
        "    hist = cv2.calcHist([img_rgb],[0],None, [256], [0,256])\n",
        "    hist= np.array(hist).flatten().tolist()\n",
        "    return hist\n",
        "\n",
        "  if histogram_greyscale_plus_all:\n",
        "    hist = cv2.calcHist([img_rgb],[0],None, [256], [0,256])\n",
        "    hist = np.array(hist).flatten().tolist()\n",
        "    img = np.reshape(img, (784*4,)).tolist()\n",
        "    [img.append(i) for i in hist]\n",
        "    return img\n",
        "  \n",
        "def data_rdd_process(data, \n",
        "                         flatten_pixels=False, \n",
        "                         infra_only=False, \n",
        "                         flatten_plus_infra=False,\n",
        "                         edges_only=False, \n",
        "                         edges_plus_pixels=False,\n",
        "                         hu_moments=False, \n",
        "                         hu_moments_plus_pixels=False,  \n",
        "                         histogram_greyscale=False, \n",
        "                         histogram_greyscale_plus_pixels=False,\n",
        "                         conv_labels=False):\n",
        "  '''\n",
        "  Input: a PySpark RDD \n",
        "  Params: flatten_pixels: if True, get the mean of each pixel (instead of separate int for each layer); ignores the infrared value (4th/last int in each pixel group), each image will total in 28x28x1 = 784 features \n",
        "          infra_only: if True, get only the infrared pixel value (which is the 4th/last value in each pixel group), each image will total in 28x28x1 = 784 features\n",
        "          flatten_plus_infra: if True, get both the mean RGB value plus the infrared pixel value, each image will total 28x28x2 = 1,568 features\n",
        "          edges_only: if True, get edges calculated with cv2.Sobel\n",
        "          edges_plus_pixels: if True, get edges and add onto the existing list of pixel data \n",
        "          hu_moments: if True, calculate Hu-moment data with cv2.HuMoment\n",
        "          hu_moments_plus_pixels: if True, add HuMoment to the existing list of pixel data\n",
        "          histogram_greyscale: if True, get greyscale histogram from cv2.CalcHist\n",
        "          histogram_greyscale_plus_pixels: if True, add greyscale histogram to existing list of pixel data\n",
        "          conv_labels: if True, map labels from [0,0,0,0] format to floats 0.0, 1.0, 2.0, or 3.0\n",
        "  Returns: a Pyspark RDD \n",
        "  '''\n",
        "  data = data.map(tuple) # remove any potential schema info, so it's only the raw data as a tuple\n",
        "\n",
        "  # assertion to check that only one major data transformation parameter is being passed\n",
        "  assert (flatten_pixels and infra_only and edges_only and edges_plus_pixels and hu_moments and hu_moments_plus_pixels and \n",
        "          histogram_greyscale and histogram_greyscale_plus_pixels and flatten_plus_infra) != True\n",
        "\n",
        "  if edges_only:\n",
        "    data = data.map(lambda row: cv2_transforms(row, sobel_edge=True))\n",
        "\n",
        "  if edges_plus_pixels:\n",
        "    data = data.map(lambda row: cv2_transforms(row, sobel_plus_all=True))\n",
        "\n",
        "  if hu_moments:\n",
        "    data = data.map(lambda row: cv2_transforms(row, hu_moments=True))\n",
        "  \n",
        "  if hu_moments_plus_pixels:\n",
        "    data = data.map(lambda row: cv2_transforms(row, hu_moments_plus_all=True))\n",
        "\n",
        "  if histogram_greyscale:\n",
        "    data = data.map(lambda row: cv2_transforms(row, histogram_greyscale=True))\n",
        "  \n",
        "  if histogram_greyscale_plus_pixels:\n",
        "    data = data.map(lambda row: cv2_transforms(row, histogram_greyscale_plus_all=True))\n",
        "\n",
        "  if flatten_pixels:\n",
        "    data = data.map(lambda row: pixels_transform(row, rgb_mean=True))\n",
        "\n",
        "  if flatten_plus_infra:\n",
        "    data = data.map(lambda row: pixels_transform(row, rgb_mean=True, add_infra=True))\n",
        "\n",
        "  if infra_only:\n",
        "    data = data.map(lambda row: pixels_transform(row, infra_only=True))\n",
        "\n",
        "  if conv_labels:\n",
        "    data = data.map(lambda row: (convert_label(row)))\n",
        "\n",
        "  return data # return a PySpark RDD\n",
        "\n",
        "\n",
        "# main data preprocessing function to pass feature extraction parameters to \n",
        "# loads in original RDD data\n",
        "def data_preprocess_main(norm_features=False, chi_sq_sel=False, chi_sq_num=300, **feature_extraction_params):\n",
        "\n",
        "  timer_start = time.perf_counter() # start timer\n",
        "\n",
        "  X_train = data_rdd_process(x_train_orig, **feature_extraction_params) \n",
        "  print(f'Loaded X_train: {X_train.count()} images')\n",
        "\n",
        "  Y_train = data_rdd_process(y_train_orig, conv_labels=True) \n",
        "  print(f'Loaded Y_train: {Y_train.count()} labels')\n",
        "\n",
        "  X_test = data_rdd_process(x_test_orig, **feature_extraction_params)\n",
        "  print(f'Loaded X_test: {X_test.count()} images')\n",
        "\n",
        "  Y_test = data_rdd_process(y_test_orig, conv_labels=True) \n",
        "  print(f'Loaded Y_test: {Y_test.count()} labels')\n",
        "\n",
        "  if norm_features:\n",
        "    nor = Normalizer(float(\"inf\"))\n",
        "    X_train = nor.transform(X_train)\n",
        "    X_test = nor.transform(X_test)\n",
        "\n",
        "  # Join training data into a single LabeledPoint RDD\n",
        "  combined_train = join_rdds_lp(Y_train, X_train)\n",
        "  print(f'Joined X_train and Y_train: {combined_train.count()} images and labels')\n",
        "\n",
        "  # ChiSqSelector for features\n",
        "  if chi_sq_sel:\n",
        "    print(f'Selecting top {chi_sq_num} features...')\n",
        "    feat_model = ChiSqSelector(numTopFeatures=chi_sq_num).fit(combined_train)\n",
        "    X_train = feat_model.transform(X_train)\n",
        "    combined_train = join_rdds_lp(Y_train, X_train)\n",
        "    X_test = feat_model.transform(X_test)\n",
        "    print('ChiSq feature selection complete.')\n",
        "\n",
        "  timer_end = time.perf_counter() # end timer\n",
        "  time_elapsed = timer_end - timer_start\n",
        "  print(f'Data pre-processing time: {time_elapsed: .3f} seconds.\\n')\n",
        "\n",
        "  return combined_train, X_train, Y_train, X_test, Y_test, time_elapsed"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCrAIb-cYva-"
      },
      "source": [
        "###########################\n",
        "##    Model Evaluation   ##\n",
        "##  Function Definition  ##\n",
        "###########################\n",
        "\n",
        "# Function to train and evaluate a MLlib RDD-based classification model\n",
        "def train_test_model(str_name, combined_train_rdd, test_images_rdd, test_labels_rdd, preprocess_time=0, get_preds_and_labels_only=False):\n",
        "\n",
        "  # Setup info for each algorithm with model_data dict\n",
        "  '''\n",
        "  type: whether it's an algo from mllib.tree or mllib.classification, because the train() functions are named differently for each\n",
        "  model: the model object to initialize prior to training\n",
        "  train_params: a dict of the parameters to use for training the model\n",
        "  '''\n",
        "  model_data = {\n",
        "      'Random Forest': {\n",
        "         'type': 'mllib_tree',\n",
        "         'model': RandomForest,\n",
        "         'train_params': {'numClasses': 4, 'categoricalFeaturesInfo': {}, 'numTrees': 100}}, # set numTrees = 100 because that's the sklearn default\n",
        "     'Decision Tree': {\n",
        "         'type': 'mllib_tree',\n",
        "          'model': DecisionTree,\n",
        "          'train_params': {'numClasses': 4, 'categoricalFeaturesInfo': {}}},\n",
        "     'Gradient Boosted Trees': {\n",
        "         'type': 'mllib_tree',\n",
        "          'model': GradientBoostedTrees,\n",
        "          'train_params': {'categoricalFeaturesInfo': {}}},\n",
        "     'Naive Bayes': {\n",
        "          'type': 'mllib_class',\n",
        "          'model': NaiveBayes,\n",
        "          'train_params': {}},\n",
        "      'Support Vector Machine': {\n",
        "          'type': 'mllib_class',\n",
        "         'model': SVMWithSGD,\n",
        "         'train_params': {'validateData': False}},    \n",
        "      'Logistic Regression': {\n",
        "         'type': 'mllib_class',\n",
        "         'model': LogisticRegressionWithLBFGS,\n",
        "         'train_params': {'numClasses': 4}}\n",
        "      }\n",
        "\n",
        "  # extract model info from model_data dict\n",
        "  alg_type = model_data[str_name]['type']\n",
        "  Model = model_data[str_name]['model']\n",
        "  train_params = model_data[str_name]['train_params']\n",
        "\n",
        "  # print the algorithm name (decorated with a border of *'s)\n",
        "  [print('*', end='') for i in range(len(str_name)+6)]\n",
        "  print(f'\\n** {str_name} **') \n",
        "  [print('*', end='') for i in range(len(str_name)+6)]\n",
        "  print('\\n', end='')\n",
        "\n",
        "  # train model\n",
        "  timer_start = time.perf_counter() # start timer for training\n",
        "  # using 'if-else' statement to handle differences between training function names for algos\n",
        "  if alg_type == 'mllib_tree':\n",
        "    out_model = Model.trainClassifier(combined_train_rdd, **train_params)\n",
        "  else:\n",
        "    out_model = Model.train(combined_train_rdd, **train_params)\n",
        "  timer_end = time.perf_counter() # end timer for training\n",
        "  train_time = timer_end - timer_start\n",
        "\n",
        "  # get predictions for model\n",
        "  timer_start = time.perf_counter() # start timer for predictions\n",
        "  predictions = out_model.predict(test_images_rdd) \n",
        "  timer_end = time.perf_counter() # stop timer for predictions\n",
        "  pred_time = timer_end - timer_start\n",
        "  total_time = preprocess_time + train_time + pred_time\n",
        "\n",
        "  # make sure prediction is a float, and add an index for joining with ground truth labels for evaluation\n",
        "  predictions = predictions.map(lambda x: float(x)).zipWithIndex().map(lambda x: (x[1], x[0]))\n",
        "  test_labels_rdd = test_labels_rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
        "  # join predictions and ground truth labels as \"test_pred\" RDD\n",
        "  test_pred = predictions.join(test_labels_rdd).sortBy(lambda x: x[0]).map(lambda x: (x[1])) \n",
        "  if get_preds_and_labels_only:\n",
        "    return test_pred\n",
        "  # get evaluation metrics for trained model, using the test_pred RDD\n",
        "  out_metrics = MulticlassMetrics(test_pred)\n",
        "  precision = out_metrics.weightedPrecision\n",
        "  recall = out_metrics.weightedRecall\n",
        "  f1score = out_metrics.weightedFMeasure()\n",
        "  accuracy = out_metrics.accuracy\n",
        "  print(f'Precision score: {precision:.2f}')\n",
        "  print(f'Recall score: {recall:.2f}')\n",
        "  print(f'F1 score: {f1score:.2f}')\n",
        "  print(f'Accuracy score: {accuracy:.2f}')\n",
        "  print(f'Pre-process time: {preprocess_time:.2f}')\n",
        "  print(f'Training time: {train_time:.2f}')\n",
        "  print(f'Prediction time: {pred_time:.2f}', end='\\n\\n')\n",
        "\n",
        "  return [str_name, precision, recall, f1score, accuracy, preprocess_time, train_time, pred_time, total_time]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgAx1afIpnGk",
        "outputId": "62fbac15-ba39-4c12-bb77-bd68baef5fcd"
      },
      "source": [
        "############################\n",
        "##     Initial MLlib      ##\n",
        "##  Algorithm Evaluation  ##\n",
        "############################\n",
        "\n",
        "# Use main pre-process function on data\n",
        "# No transformations on data (except for required joins, etc.) for initial algorithm evaluation\n",
        "combined_train, X_train, Y_train, X_test, Y_test, preprocess_time = data_preprocess_main()\n",
        "\n",
        "# train and test algorithms and output data to a .csv file\n",
        "output_data = []\n",
        "testing_list = ['Decision Tree', 'Random Forest', 'Gradient Boosted Trees', 'Naive Bayes', 'Support Vector Machine', 'Logistic Regression']\n",
        "for alg in testing_list:\n",
        "  output_data.append(train_test_model(alg, combined_train, X_test, Y_test, preprocess_time))\n",
        "df = pd.DataFrame(output_data)\n",
        "df.columns = ['Algorithm Name', 'Precision Score', 'Recall Score', 'F1 Score', 'Accuracy Score', 'Pre-Processing Time', 'Training Time', 'Prediction Time', 'Total Time']\n",
        "df = df.sort_values(by='F1 Score', ascending=False)\n",
        "df.to_csv('1_initial_testing_output_data.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  184.742 seconds.\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.77\n",
            "Recall score: 0.76\n",
            "F1 score: 0.76\n",
            "Accuracy score: 0.76\n",
            "Pre-process time: 184.74\n",
            "Training time: 73.27\n",
            "Prediction time: 0.03\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.82\n",
            "Recall score: 0.82\n",
            "F1 score: 0.81\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 184.74\n",
            "Training time: 90.40\n",
            "Prediction time: 0.03\n",
            "\n",
            "****************************\n",
            "** Gradient Boosted Trees **\n",
            "****************************\n",
            "Precision score: 0.31\n",
            "Recall score: 0.44\n",
            "F1 score: 0.33\n",
            "Accuracy score: 0.44\n",
            "Pre-process time: 184.74\n",
            "Training time: 1137.21\n",
            "Prediction time: 0.02\n",
            "\n",
            "*****************\n",
            "** Naive Bayes **\n",
            "*****************\n",
            "Precision score: 0.57\n",
            "Recall score: 0.51\n",
            "F1 score: 0.51\n",
            "Accuracy score: 0.51\n",
            "Pre-process time: 184.74\n",
            "Training time: 37.97\n",
            "Prediction time: 0.00\n",
            "\n",
            "****************************\n",
            "** Support Vector Machine **\n",
            "****************************\n",
            "Precision score: 0.04\n",
            "Recall score: 0.20\n",
            "F1 score: 0.07\n",
            "Accuracy score: 0.20\n",
            "Pre-process time: 184.74\n",
            "Training time: 54.88\n",
            "Prediction time: 0.00\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.73\n",
            "Recall score: 0.74\n",
            "F1 score: 0.73\n",
            "Accuracy score: 0.74\n",
            "Pre-process time: 184.74\n",
            "Training time: 3114.12\n",
            "Prediction time: 0.00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApGrFHOYUyzL",
        "outputId": "9138cfcb-23d3-4ee4-fdc6-fc57f26e0109"
      },
      "source": [
        "################################################################\n",
        "##             Testing Feature Extraction Methods             ##\n",
        "##  with Random Forest, Decision Tree, & Logistic Regression  ##\n",
        "################################################################\n",
        "\n",
        "# Setup parameter options to enumerate through\n",
        "feature_extraction_params = ['hu_moments_plus_pixels', 'hu_moments', 'histogram_greyscale', 'histogram_greyscale_plus_pixels',\n",
        "                               'flatten_pixels', 'infra_only', 'flatten_plus_infra', 'edges_only', 'edges_plus_pixels']\n",
        "\n",
        "output_data = []\n",
        "for i in range(len(feature_extraction_params)):\n",
        "  p = feature_extraction_params[i]\n",
        "  print(f'Testing with feature extraction parameter: {p}')\n",
        "  tmp_feature_extraction_param = {}\n",
        "  tmp_feature_extraction_param[p] = True \n",
        "  # Use main pre-process function on data\n",
        "  combined_train, X_train, Y_train, X_test, Y_test, preprocess_time = data_preprocess_main(**tmp_feature_extraction_param)\n",
        "\n",
        "  for algo in ['Random Forest', 'Decision Tree', 'Logistic Regression']:\n",
        "    # train and test algorithms and append to output_data\n",
        "    output_data.append(train_test_model(algo, combined_train, X_test, Y_test, preprocess_time))\n",
        "    output_data[-1].insert(1, p) # add the feature extraction method string to the output\n",
        "\n",
        "df = pd.DataFrame(output_data)\n",
        "df.columns = ['Algorithm Name', 'Feature Extraction Method', 'Precision Score', 'Recall Score', 'F1 Score', 'Accuracy Score', 'Pre-Processing Time', 'Training Time', 'Prediction Time', 'Total Time']\n",
        "df = df.sort_values(by='F1 Score', ascending=False)\n",
        "df.to_csv(f'2_testing_output_feature_extraction.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with feature extraction parameter: hu_moments_plus_pixels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  627.179 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.83\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.83\n",
            "Pre-process time: 627.18\n",
            "Training time: 144.35\n",
            "Prediction time: 0.04\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.77\n",
            "Recall score: 0.77\n",
            "F1 score: 0.76\n",
            "Accuracy score: 0.77\n",
            "Pre-process time: 627.18\n",
            "Training time: 117.33\n",
            "Prediction time: 0.03\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.76\n",
            "Recall score: 0.76\n",
            "F1 score: 0.76\n",
            "Accuracy score: 0.76\n",
            "Pre-process time: 627.18\n",
            "Training time: 3640.47\n",
            "Prediction time: 0.00\n",
            "\n",
            "Testing with feature extraction parameter: hu_moments\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  382.974 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.73\n",
            "Recall score: 0.72\n",
            "F1 score: 0.71\n",
            "Accuracy score: 0.72\n",
            "Pre-process time: 382.97\n",
            "Training time: 9.47\n",
            "Prediction time: 0.03\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.73\n",
            "Recall score: 0.73\n",
            "F1 score: 0.71\n",
            "Accuracy score: 0.73\n",
            "Pre-process time: 382.97\n",
            "Training time: 2.65\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.47\n",
            "Recall score: 0.44\n",
            "F1 score: 0.37\n",
            "Accuracy score: 0.44\n",
            "Pre-process time: 382.97\n",
            "Training time: 19.03\n",
            "Prediction time: 0.00\n",
            "\n",
            "Testing with feature extraction parameter: histogram_greyscale\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  360.920 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.83\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.83\n",
            "Pre-process time: 360.92\n",
            "Training time: 23.34\n",
            "Prediction time: 0.03\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.85\n",
            "Recall score: 0.86\n",
            "F1 score: 0.85\n",
            "Accuracy score: 0.86\n",
            "Pre-process time: 360.92\n",
            "Training time: 6.11\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.93\n",
            "Recall score: 0.92\n",
            "F1 score: 0.93\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 360.92\n",
            "Training time: 230.62\n",
            "Prediction time: 0.00\n",
            "\n",
            "Testing with feature extraction parameter: histogram_greyscale_plus_pixels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  469.699 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.85\n",
            "Recall score: 0.83\n",
            "F1 score: 0.83\n",
            "Accuracy score: 0.83\n",
            "Pre-process time: 469.70\n",
            "Training time: 110.88\n",
            "Prediction time: 0.03\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.88\n",
            "Recall score: 0.87\n",
            "F1 score: 0.88\n",
            "Accuracy score: 0.87\n",
            "Pre-process time: 469.70\n",
            "Training time: 73.27\n",
            "Prediction time: 0.28\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.92\n",
            "F1 score: 0.92\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 469.70\n",
            "Training time: 3879.05\n",
            "Prediction time: 0.00\n",
            "\n",
            "Testing with feature extraction parameter: flatten_pixels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  454.396 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.58\n",
            "Recall score: 0.69\n",
            "F1 score: 0.62\n",
            "Accuracy score: 0.69\n",
            "Pre-process time: 454.40\n",
            "Training time: 34.67\n",
            "Prediction time: 0.03\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.67\n",
            "Recall score: 0.68\n",
            "F1 score: 0.67\n",
            "Accuracy score: 0.68\n",
            "Pre-process time: 454.40\n",
            "Training time: 16.99\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.33\n",
            "Recall score: 0.30\n",
            "F1 score: 0.23\n",
            "Accuracy score: 0.30\n",
            "Pre-process time: 454.40\n",
            "Training time: 928.47\n",
            "Prediction time: 0.00\n",
            "\n",
            "Testing with feature extraction parameter: infra_only\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  262.849 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.55\n",
            "Recall score: 0.61\n",
            "F1 score: 0.53\n",
            "Accuracy score: 0.61\n",
            "Pre-process time: 262.85\n",
            "Training time: 34.00\n",
            "Prediction time: 0.03\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.49\n",
            "Recall score: 0.59\n",
            "F1 score: 0.52\n",
            "Accuracy score: 0.59\n",
            "Pre-process time: 262.85\n",
            "Training time: 30.67\n",
            "Prediction time: 0.34\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.15\n",
            "Recall score: 0.28\n",
            "F1 score: 0.17\n",
            "Accuracy score: 0.28\n",
            "Pre-process time: 262.85\n",
            "Training time: 930.66\n",
            "Prediction time: 0.00\n",
            "\n",
            "Testing with feature extraction parameter: flatten_plus_infra\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  597.137 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.79\n",
            "Recall score: 0.79\n",
            "F1 score: 0.78\n",
            "Accuracy score: 0.79\n",
            "Pre-process time: 597.14\n",
            "Training time: 80.27\n",
            "Prediction time: 0.03\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.73\n",
            "Recall score: 0.73\n",
            "F1 score: 0.73\n",
            "Accuracy score: 0.73\n",
            "Pre-process time: 597.14\n",
            "Training time: 56.56\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.46\n",
            "Recall score: 0.46\n",
            "F1 score: 0.46\n",
            "Accuracy score: 0.46\n",
            "Pre-process time: 597.14\n",
            "Training time: 1760.18\n",
            "Prediction time: 0.00\n",
            "\n",
            "Testing with feature extraction parameter: edges_only\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  373.029 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.29\n",
            "Recall score: 0.46\n",
            "F1 score: 0.35\n",
            "Accuracy score: 0.46\n",
            "Pre-process time: 373.03\n",
            "Training time: 28.30\n",
            "Prediction time: 0.03\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.38\n",
            "Recall score: 0.44\n",
            "F1 score: 0.38\n",
            "Accuracy score: 0.44\n",
            "Pre-process time: 373.03\n",
            "Training time: 15.60\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.14\n",
            "Recall score: 0.35\n",
            "F1 score: 0.19\n",
            "Accuracy score: 0.35\n",
            "Pre-process time: 373.03\n",
            "Training time: 774.37\n",
            "Prediction time: 0.00\n",
            "\n",
            "Testing with feature extraction parameter: edges_plus_pixels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  717.068 seconds.\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 717.07\n",
            "Training time: 144.17\n",
            "Prediction time: 0.04\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.77\n",
            "Recall score: 0.76\n",
            "F1 score: 0.76\n",
            "Accuracy score: 0.76\n",
            "Pre-process time: 717.07\n",
            "Training time: 81.04\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.88\n",
            "Recall score: 0.88\n",
            "F1 score: 0.88\n",
            "Accuracy score: 0.88\n",
            "Pre-process time: 717.07\n",
            "Training time: 4500.32\n",
            "Prediction time: 0.00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_nJMoSv8Zan",
        "outputId": "7c5a888c-c6dc-480d-fdd3-df14c86d53b1"
      },
      "source": [
        "################################################################################\n",
        "##       Retest all algos to confirm if Logistic Regression is the best       ##\n",
        "##  performing algorithm when using 'histogram_greyscale' feature extraction  ##\n",
        "################################################################################\n",
        "\n",
        "# Use main pre-process function on data with greyscale histogram feature extraction\n",
        "feat_extract = {'histogram_greyscale': True}\n",
        "combined_train, X_train, Y_train, X_test, Y_test, preprocess_time = data_preprocess_main(**feat_extract)\n",
        "\n",
        "# train and test algorithms and output data to a .csv file\n",
        "output_data = []\n",
        "testing_list = ['Decision Tree', 'Random Forest', 'Gradient Boosted Trees', 'Naive Bayes', 'Support Vector Machine', 'Logistic Regression']\n",
        "for alg in testing_list:\n",
        "  output_data.append(train_test_model(alg, combined_train, X_test, Y_test, preprocess_time))\n",
        "df = pd.DataFrame(output_data)\n",
        "df.columns = ['Algorithm Name', 'Precision Score', 'Recall Score', 'F1 Score', 'Accuracy Score', 'Pre-Processing Time', 'Training Time', 'Prediction Time', 'Total Time']\n",
        "df = df.sort_values(by='F1 Score', ascending=False)\n",
        "df.to_csv('3_retest_histogram_greyscale.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  384.123 seconds.\n",
            "\n",
            "*******************\n",
            "** Decision Tree **\n",
            "*******************\n",
            "Precision score: 0.85\n",
            "Recall score: 0.86\n",
            "F1 score: 0.85\n",
            "Accuracy score: 0.86\n",
            "Pre-process time: 384.12\n",
            "Training time: 9.17\n",
            "Prediction time: 0.04\n",
            "\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.83\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.83\n",
            "Pre-process time: 384.12\n",
            "Training time: 16.85\n",
            "Prediction time: 0.03\n",
            "\n",
            "****************************\n",
            "** Gradient Boosted Trees **\n",
            "****************************\n",
            "Precision score: 0.31\n",
            "Recall score: 0.44\n",
            "F1 score: 0.33\n",
            "Accuracy score: 0.44\n",
            "Pre-process time: 384.12\n",
            "Training time: 89.76\n",
            "Prediction time: 0.03\n",
            "\n",
            "*****************\n",
            "** Naive Bayes **\n",
            "*****************\n",
            "Precision score: 0.75\n",
            "Recall score: 0.73\n",
            "F1 score: 0.73\n",
            "Accuracy score: 0.73\n",
            "Pre-process time: 384.12\n",
            "Training time: 7.21\n",
            "Prediction time: 0.00\n",
            "\n",
            "****************************\n",
            "** Support Vector Machine **\n",
            "****************************\n",
            "Precision score: 0.30\n",
            "Recall score: 0.39\n",
            "F1 score: 0.29\n",
            "Accuracy score: 0.39\n",
            "Pre-process time: 384.12\n",
            "Training time: 12.58\n",
            "Prediction time: 0.00\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.93\n",
            "Recall score: 0.92\n",
            "F1 score: 0.93\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 384.12\n",
            "Training time: 244.22\n",
            "Prediction time: 0.00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVb3uFvcEPGh",
        "outputId": "2243ce0e-449a-433c-e6ad-16408c141c7e"
      },
      "source": [
        "###########################################################################\n",
        "##                   Further Refinement of Best Model                    ## \n",
        "##  (Logistic Regression with 'histogram greyscale' feature extraction)  ##\n",
        "##             with Data Normalization and Feature Selection             ##\n",
        "###########################################################################\n",
        "\n",
        "feature_extraction_param = {'histogram_greyscale': True}\n",
        "\n",
        "output_data = []\n",
        "\n",
        "# Test Normalizing features\n",
        "for param in ['Normalize Features', 'ChiSq Feature Selector']:\n",
        " \n",
        "  if param == 'Normalize Features':\n",
        "    combined_train, X_train, Y_train, X_test, Y_test, preprocess_time = data_preprocess_main(norm_features=True, **feature_extraction_param)\n",
        "    output_data.append(['Normalize Features', 'N/A'])\n",
        "    [output_data[-1].append(i) for i in train_test_model('Logistic Regression', combined_train, X_test, Y_test, preprocess_time)]\n",
        "    output_data[-1].insert(3, 'histogram_greyscale')\n",
        "\n",
        "  else:\n",
        "    chi_sq_range = [200, 100]\n",
        "    for int in chi_sq_range:\n",
        "      combined_train, X_train, Y_train, X_test, Y_test, preprocess_time = data_preprocess_main(chi_sq_sel=True, chi_sq_num=int, **feature_extraction_param)\n",
        "      output_data.append(['ChiSq Selection', int])\n",
        "      [output_data[-1].append(i) for i in train_test_model('Logistic Regression', combined_train, X_test, Y_test, preprocess_time)]\n",
        "      output_data[-1].insert(3, 'histogram_greyscale')\n",
        "\n",
        "df = pd.DataFrame(output_data)\n",
        "df.columns = ['New Param', 'ChiSq Num', 'Algorithm Name', 'Feature Extraction Method', 'Precision Score', 'Recall Score', 'F1 Score', 'Accuracy Score', 'Pre-Processing Time', 'Training Time', 'Prediction Time', 'Total Time']\n",
        "df = df.sort_values(by='F1 Score', ascending=False)\n",
        "df.to_csv(f'4_testing_output_extra_params.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  383.302 seconds.\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.93\n",
            "Recall score: 0.93\n",
            "F1 score: 0.93\n",
            "Accuracy score: 0.93\n",
            "Pre-process time: 383.30\n",
            "Training time: 225.86\n",
            "Prediction time: 0.00\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Selecting top 200 features...\n",
            "ChiSq feature selection complete.\n",
            "Data pre-processing time:  490.548 seconds.\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.91\n",
            "F1 score: 0.91\n",
            "Accuracy score: 0.91\n",
            "Pre-process time: 490.55\n",
            "Training time: 289.20\n",
            "Prediction time: 0.00\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Selecting top 100 features...\n",
            "ChiSq feature selection complete.\n",
            "Data pre-processing time:  490.968 seconds.\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.73\n",
            "Recall score: 0.75\n",
            "F1 score: 0.73\n",
            "Accuracy score: 0.75\n",
            "Pre-process time: 490.97\n",
            "Training time: 201.15\n",
            "Prediction time: 0.00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "##                 10-Fold Cross Validation                    ## \n",
        "##             and Paired T-Test of Best Model:                ##\n",
        "##  normalized, greyscale histogram, with Logistic Regression  ##\n",
        "#################################################################\n",
        "\n",
        "# Function for preprocessing data for cross-validation\n",
        "def crossVal_preprocess(seed, norm=False, **feature_extraction_param):\n",
        "  if norm:\n",
        "    nor = Normalizer(float(\"inf\")) # initialize Normalizer\n",
        "\n",
        "  X_train = data_rdd_process(x_train_orig, **feature_extraction_param)\n",
        "  if norm:\n",
        "    X_train = nor.transform(X_train)\n",
        "  print(f'Loaded X_train: {X_train.count()} images')\n",
        "\n",
        "  Y_train = data_rdd_process(y_train_orig, conv_labels=True) \n",
        "  print(f'Loaded Y_train: {Y_train.count()} labels')\n",
        "\n",
        "  X_test = data_rdd_process(x_test_orig, **feature_extraction_param)\n",
        "  if norm:\n",
        "    X_test = nor.transform(X_test)\n",
        "  print(f'Loaded X_test: {X_test.count()} images')\n",
        "\n",
        "  Y_test = data_rdd_process(y_test_orig, conv_labels=True) \n",
        "  print(f'Loaded Y_test: {Y_test.count()} labels')\n",
        "\n",
        "  # Combine datasets into a single PySpark RDD\n",
        "  all_data_x = X_train.union(X_test).zipWithIndex().map(lambda x: (x[1], x[0]))\n",
        "  all_data_y = Y_train.union(Y_test).zipWithIndex().map(lambda x: (x[1], x[0]))\n",
        "  all_data_rdd = all_data_y.join(all_data_x).sortBy(lambda x: x[0]) # sort and leave index after joining\n",
        "  print(f'Joined all data: {all_data_rdd.count()} images with labels')\n",
        "\n",
        "  # Split combined RDD into train and test sets, using seed number\n",
        "  train_rdd, test_rdd = all_data_rdd.randomSplit([0.8, 0.2], seed=seed)\n",
        "\n",
        "  # Convert train_rdd into LabeledPoint format, sort (to ensure order after splitting), and remove index\n",
        "  train_rdd = train_rdd.mapValues(lambda x: LabeledPoint(x[0], Vectors.dense(x[1]))).sortBy(lambda x: x[0]).map(lambda x: x[1])\n",
        "\n",
        "  # Convert test_rdd into test_labels_rdd and test_images_rdd\n",
        "  # Sort then remove index for each\n",
        "  test_labels_rdd = test_rdd.mapValues(lambda x: x[0]).sortBy(lambda x: x[0]).map(lambda x: x[1])\n",
        "  test_images_rdd = test_rdd.mapValues(lambda x: x[1]).sortBy(lambda x: x[0]).map(lambda x: x[1])\n",
        "\n",
        "  return train_rdd, test_labels_rdd, test_images_rdd\n",
        "\n",
        "baseline_scores = []\n",
        "best_model_scores = []\n",
        "seeds = [44,32,883,12333,5,98,623,1,57,42] # using seed for randomSplit on RDDs (to ensure the same sets of data are evaluated for each model)\n",
        "for fold in range(10):\n",
        "\n",
        "  seed = seeds[fold] # set seed value for fold iteration\n",
        "\n",
        "  # combine all data for unmodified images\n",
        "  baseline_train, baseline_test_labels, baseline_test_images = crossVal_preprocess(seed) \n",
        "\n",
        "  # combine all data with feature extraction\n",
        "  feature_extraction_param = {'histogram_greyscale': True}\n",
        "  best_model_train, best_model_test_labels, best_model_test_images = crossVal_preprocess(seed,norm=True, **feature_extraction_param)\n",
        "\n",
        "  # Evaluations return [str_name, precision, recall, f1score, accuracy, preprocess_time, train_time, pred_time, total_time]\n",
        "  baseline_eval = train_test_model('Random Forest', baseline_train, baseline_test_images, baseline_test_labels)\n",
        "  best_model_eval = train_test_model('Logistic Regression', best_model_train, best_model_test_images, best_model_test_labels)\n",
        "  \n",
        "  baseline_f1 = baseline_eval[3]\n",
        "  best_model_f1 = best_model_eval[3]\n",
        "  print(f'Fold {fold}:{chr(10)}Baseline model weighted-F1 score: {baseline_f1}{chr(10)}Best Model weighted-F1 score: {best_model_f1}{chr(10)}')\n",
        "  baseline_scores.append(baseline_f1)\n",
        "  best_model_scores.append(best_model_f1)\n",
        "\n",
        "# Compute paired t-test\n",
        "test_result = ttest_rel(baseline_scores, best_model_scores)\n",
        "t_stat = test_result[0]\n",
        "p_val = test_result[1]\n",
        "print(f'T-test statistic: {t_stat}{chr(10)}P-value: {p_val}')\n",
        "\n",
        "# Create Pandas dataframe from metrics and export to csv \n",
        "all_metrics = {'Baseline Model': baseline_scores, 'Best Model': best_model_scores}   \n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "metrics_df.to_csv('cross_validation_metrics.csv', index=False)\n",
        "\n",
        "# Create Pandas dataframe for t-test statistics and export to csv\n",
        "stat_df = pd.DataFrame([[t_stat, p_val]], columns=['Statistic', 'P-Value'])\n",
        "stat_df.to_csv('paired_t_test_statistics.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFfp8Nhn_HF5",
        "outputId": "4057c958-5afd-47e6-fabe-aa8dded85dfe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 99.16\n",
            "Prediction time: 0.03\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.93\n",
            "Recall score: 0.93\n",
            "F1 score: 0.93\n",
            "Accuracy score: 0.93\n",
            "Pre-process time: 0.00\n",
            "Training time: 149.74\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 0:\n",
            "Baseline model weighted-F1 score: 0.8166619604402402\n",
            "Best Model weighted-F1 score: 0.925250882045282\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 92.06\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.92\n",
            "F1 score: 0.92\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 0.00\n",
            "Training time: 141.99\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 1:\n",
            "Baseline model weighted-F1 score: 0.8159047605130252\n",
            "Best Model weighted-F1 score: 0.9237031791694654\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 87.50\n",
            "Prediction time: 0.03\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.92\n",
            "F1 score: 0.92\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 0.00\n",
            "Training time: 142.86\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 2:\n",
            "Baseline model weighted-F1 score: 0.8173134908722409\n",
            "Best Model weighted-F1 score: 0.9240745967307054\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.82\n",
            "Recall score: 0.82\n",
            "F1 score: 0.81\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 89.83\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.92\n",
            "F1 score: 0.92\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 0.00\n",
            "Training time: 143.53\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 3:\n",
            "Baseline model weighted-F1 score: 0.8134623514905691\n",
            "Best Model weighted-F1 score: 0.9237956258399782\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 89.05\n",
            "Prediction time: 0.04\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.92\n",
            "F1 score: 0.92\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 0.00\n",
            "Training time: 149.35\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 4:\n",
            "Baseline model weighted-F1 score: 0.8154855980095859\n",
            "Best Model weighted-F1 score: 0.9234902084013472\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 92.20\n",
            "Prediction time: 0.02\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.93\n",
            "Recall score: 0.93\n",
            "F1 score: 0.93\n",
            "Accuracy score: 0.93\n",
            "Pre-process time: 0.00\n",
            "Training time: 147.04\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 5:\n",
            "Baseline model weighted-F1 score: 0.819041247720875\n",
            "Best Model weighted-F1 score: 0.9251158858396704\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.82\n",
            "Recall score: 0.82\n",
            "F1 score: 0.81\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 94.04\n",
            "Prediction time: 0.03\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.92\n",
            "F1 score: 0.92\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 0.00\n",
            "Training time: 143.10\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 6:\n",
            "Baseline model weighted-F1 score: 0.8132619092429608\n",
            "Best Model weighted-F1 score: 0.92316177584449\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 86.15\n",
            "Prediction time: 0.34\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.92\n",
            "F1 score: 0.92\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 0.00\n",
            "Training time: 148.64\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 7:\n",
            "Baseline model weighted-F1 score: 0.8162747027949364\n",
            "Best Model weighted-F1 score: 0.9228038086897267\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 89.32\n",
            "Prediction time: 0.03\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.93\n",
            "Recall score: 0.93\n",
            "F1 score: 0.93\n",
            "Accuracy score: 0.93\n",
            "Pre-process time: 0.00\n",
            "Training time: 165.47\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 8:\n",
            "Baseline model weighted-F1 score: 0.8162176723920114\n",
            "Best Model weighted-F1 score: 0.9253954201650453\n",
            "\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined all data: 500000 images with labels\n",
            "*******************\n",
            "** Random Forest **\n",
            "*******************\n",
            "Precision score: 0.83\n",
            "Recall score: 0.82\n",
            "F1 score: 0.82\n",
            "Accuracy score: 0.82\n",
            "Pre-process time: 0.00\n",
            "Training time: 83.42\n",
            "Prediction time: 1.93\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "Precision score: 0.92\n",
            "Recall score: 0.92\n",
            "F1 score: 0.92\n",
            "Accuracy score: 0.92\n",
            "Pre-process time: 0.00\n",
            "Training time: 147.04\n",
            "Prediction time: 0.00\n",
            "\n",
            "Fold 9:\n",
            "Baseline model weighted-F1 score: 0.8156709041381587\n",
            "Best Model weighted-F1 score: 0.9229030257601218\n",
            "\n",
            "T-test statistic: -236.34290816523105\n",
            "P-value: 2.2115737020987295e-18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqY8n0-w93R3",
        "outputId": "5db70999-381b-4214-c637-356096cb620b"
      },
      "source": [
        "############################################################################################################################\n",
        "##  Save predictions from best model, ground truth labels, original image data, and greyscale histogram transformed data  ##\n",
        "############################################################################################################################\n",
        "\n",
        "# Use main pre-process function on data\n",
        "feat_extract = {'histogram_greyscale': True}\n",
        "combined_train, X_train, Y_train, X_test, Y_test, preprocess_time = data_preprocess_main(norm_features=True, **feat_extract)\n",
        "\n",
        "# Train model, get predictions, and save to a .csv file\n",
        "# Get predictions with labels\n",
        "preds_labels = train_test_model('Logistic Regression', combined_train, X_test, Y_test, preprocess_time, get_preds_and_labels_only=True)\n",
        "idx_preds_labels = preds_labels.zipWithIndex().map(lambda x: (x[1], (x[0][0], x[0][1]))) # add index\n",
        "\n",
        "idx_x_test_orig = x_test_orig.zipWithIndex().map(lambda x: (x[1], x[0])).mapValues(list) # add index\n",
        "print(x_test_orig.take)\n",
        "idx_X_test = X_test.zipWithIndex().map(lambda x: (x[1], x[0])) # add index\n",
        "preds_labels_X = idx_preds_labels.join(idx_x_test_orig).map(lambda x: (x[0], (x[1][0][0], x[1][0][1], x[1][1]))) # join preds, labels, and orig_x_test\n",
        "preds_labels_X = preds_labels_X.join(idx_X_test).sortBy(lambda x: x[0]).map(lambda x: [x[1][0][0], x[1][0][1], x[1][0][2], x[1][1]]) # join with transformed X_test\n",
        "preds_labels_X = preds_labels_X.collect()\n",
        "df = pd.DataFrame(preds_labels_X, columns=['Predicted','Actual','Orig Image', 'Greyscale Histogram Transformed'])\n",
        "print(df['Orig Image'].head())\n",
        "df.to_csv('6_predictions.csv', index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded X_train: 400000 images\n",
            "Loaded Y_train: 400000 labels\n",
            "Loaded X_test: 100000 images\n",
            "Loaded Y_test: 100000 labels\n",
            "Joined X_train and Y_train: 400000 images and labels\n",
            "Data pre-processing time:  376.718 seconds.\n",
            "\n",
            "*************************\n",
            "** Logistic Regression **\n",
            "*************************\n",
            "<bound method RDD.take of MapPartitionsRDD[44] at javaToPython at NativeMethodAccessorImpl.java:0>\n",
            "0    [130, 120, 113, 143, 137, 134, 127, 159, 133, ...\n",
            "1    [164, 139, 111, 196, 171, 147, 122, 201, 168, ...\n",
            "2    [120, 114, 114, 132, 111, 105, 100, 128, 61, 4...\n",
            "3    [126, 121, 99, 166, 129, 122, 106, 167, 127, 1...\n",
            "4    [92, 90, 65, 175, 92, 94, 73, 176, 100, 104, 8...\n",
            "Name: Orig Image, dtype: object\n"
          ]
        }
      ]
    }
  ]
}